# Modified official set of upstream alerts https://github.com/ceph/ceph/blob/octopus/monitoring/prometheus/alerts/ceph_default_alerts.yml

{% raw %}

groups:
- name: health
  rules:
  - alert: HealthError
    expr: ceph_health_status == 2
    for: 5m
    labels:
      severity: critical
    annotations:
      description: >
        Ceph in HEALTH_ERROR state for more than 5 minutes.
        Please check "ceph health detail" for more information.

  - alert: HealthWarn
    expr: ceph_health_status == 1
    for: 15m
    labels:
      severity: warning
    annotations:
      description: >
        Ceph has been in HEALTH_WARN for more than 15 minutes.
        Please check "ceph health detail" for more information.

- name: mon
  rules:
  - alert: LowMonitorQuorumCount
    expr: sum(ceph_mon_quorum_status) < 3
    labels:
      severity: critical
    annotations:
      description: |
        Monitor count in quorum is below three.

        Only {{ $value }} of {{ with query "count(ceph_mon_quorum_status)" }}{{ . | first | value }}{{ end }} monitors are active.

        The following monitors are down:
        {{- range query "(ceph_mon_quorum_status == 0) + on(ceph_daemon) group_left(hostname) (ceph_mon_metadata * 0)" }}
          - {{ .Labels.ceph_daemon }} on {{ .Labels.hostname }}
        {{- end }}

- name: osd
  rules:
  - alert: 10percentOSDsDown
    expr: count(ceph_osd_up == 0) / count(ceph_osd_up) * 100 >= 10
    labels:
      severity: critical
    annotations:
      description: |
        {{ $value | humanize }}% or {{ with query "count(ceph_osd_up == 0)" }}{{ . | first | value }}{{ end }} of {{ with query "count(ceph_osd_up)" }}{{ . | first | value }}{{ end }} OSDs are down (â‰¥ 10%).

        The following OSDs are down:
        {{- range query "(ceph_osd_up * on(ceph_daemon) group_left(hostname) ceph_osd_metadata) == 0" }}
          - {{ .Labels.ceph_daemon }} on {{ .Labels.hostname }}
        {{- end }}

  - alert: OSDDdown
    expr: count(ceph_osd_up == 0) > 0
    for: 15m
    labels:
      severity: warning
    annotations:
      description: |
        {{ $s := "" }}{{ if gt $value 1.0 }}{{ $s = "s" }}{{ end }}
        {{ $value }} OSD{{ $s }} down for more than 15 minutes.

        {{ $value }} of {{ query "count(ceph_osd_up)" | first | value }} OSDs are down.

        The following OSD{{ $s }} {{ if eq $s "" }}is{{ else }}are{{ end }} down:
          {{- range query "(ceph_osd_up * on(ceph_daemon) group_left(hostname) ceph_osd_metadata) == 0"}}
          - {{ .Labels.ceph_daemon }} on {{ .Labels.hostname }}
          {{- end }}

  - alert: OSDsNearFull
    expr: |
      (
      ((ceph_osd_stat_bytes_used / ceph_osd_stat_bytes) and on(ceph_daemon) ceph_osd_up == 1)
      * on(ceph_daemon) group_left(hostname) ceph_osd_metadata
      ) * 100 > 90
    for: 5m
    labels:
      severity: critical
    annotations:
      description: >
        OSD {{ $labels.ceph_daemon }} on {{ $labels.hostname }} is
        dangerously full: {{ $value | humanize }}%

  - alert: FlappingOSD
    expr: |
      (
      rate(ceph_osd_up[5m])
      * on(ceph_daemon) group_left(hostname) ceph_osd_metadata
      ) * 60 > 1
    labels:
      severity: warning
    annotations:
      description: >
        OSD {{ $labels.ceph_daemon }} on {{ $labels.hostname }} was
        marked down and back up at {{ $value | humanize }} times once a
        minute for 5 minutes.

- name: pgs
  rules:
  - alert: PgsInactive
    expr: ceph_pool_metadata * on(pool_id,instance) group_left() (ceph_pg_total - ceph_pg_active) > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      description: >
        {{ $value }} PGs have been inactive for more than 5 minutes in pool {{ $labels.name }}.
        Inactive placement groups aren't able to serve read/write
        requests.

  - alert: PgsUnclean
    expr: ceph_pool_metadata * on(pool_id,instance) group_left() (ceph_pg_total - ceph_pg_clean) > 0
    for: 15m
    labels:
      severity: warning
    annotations:
      description: >
        {{ $value }} PGs haven't been clean for more than 15 minutes in pool {{ $labels.name }}.
        Unclean PGs haven't been able to completely recover from a
        previous failure.

- name: nodes
  rules:
  - alert: RootVolumeFull
    expr: node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"} * 100 < 5
    for: 5m
    labels:
      severity: critical
    annotations:
      description: "Root volume (OSD and MON store) is dangerously full: {{ $value | humanize }}% free."

  # alert on nic packet errors and drops rates > 1 packet/s
  - alert: NetworkPacketsDropped
    expr: irate(node_network_receive_drop_total{device!~"lo|br.*|.*-ovs|tap.*"}[5m]) + irate(node_network_transmit_drop_total{device!~"lo|br.*|.*-ovs|tap.*"}[5m]) > 1
    labels:
      severity: warning
    annotations:
      description: >
        Node {{ $labels.instance }} experiences packet drop > 1
        packet/s on interface {{ $labels.device }}.

  - alert: NetworkPacketErrors
    expr: |
      irate(node_network_receive_errs_total{device!="lo"}[5m]) +
      irate(node_network_transmit_errs_total{device!="lo"}[5m]) > 1
    labels:
      severity: warning
    annotations:
      description: >
        Node {{ $labels.instance }} experiences packet errors > 1
        packet/s on interface {{ $labels.device }}.

  - alert: StorageFillingUp
    expr: |
      predict_linear(node_filesystem_avail_bytes[2d], 3600 * 24 * 5) *
      on(instance) group_left(nodename) node_uname_info < 0
    labels:
      severity: warning
    annotations:
      description: >
        Mountpoint {{ $labels.mountpoint }} on {{ $labels.nodename }}
        will be full in less than 5 days assuming the average fill-up
        rate of the past 48 hours.

- name: pools
  rules:
  - alert: PoolFull
    expr: |
      ceph_pool_stored / (ceph_pool_stored + ceph_pool_max_avail)
      * on(pool_id) group_right ceph_pool_metadata * 100 > 90
    labels:
      severity: critical
    annotations:
      description: "Pool {{ $labels.name }} at {{ $value | humanize }}% capacity."

  - alert: PoolFillingUp
    expr: |
      (
      predict_linear(ceph_pool_stored[2d], 3600 * 24 * 5)
      >= ceph_pool_stored + ceph_pool_max_avail
      ) * on(pool_id) group_left(name) ceph_pool_metadata
    labels:
      severity: warning
    annotations:
      description: >
        Pool {{ $labels.name }} will be full in less than 5 days
        assuming the average fill-up rate of the past 48 hours.

- name: healthchecks
  rules:
  - alert: SlowOSDOps
    expr: ceph_healthcheck_slow_ops > 0
    for: 30s
    labels:
      severity: warning
    annotations:
      description: "{{ $value }} OSD requests are taking too long to process (osd_op_complaint_time exceeded)"

{% endraw %}
